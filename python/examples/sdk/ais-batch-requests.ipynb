{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2d864c2",
   "metadata": {},
   "source": [
    "# Batch Requests with AIStore: Full Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70439db4",
   "metadata": {},
   "source": [
    "The GetBatch API is a high-performance data retrieval interface that allows clients to efficiently fetch data from multiple objects in a single HTTP request rather than making individual requests for each object. This batching approach is particularly valuable for applications that need to retrieve large numbers of objects and/or files within archive contents.\n",
    "\n",
    "The API works by accepting a batch request containing multiple object specifications (including optional parameters like archive paths for extracting specific files from archives, byte ranges for partial retrieval, and custom metadata), then processing these requests on the server side. The response can be delivered as either a streaming archive containing all requested files, or as a structured multipart response that includes both metadata about each object and the actual file contents, allowing clients to process results efficiently while maintaining detailed information about each retrieved item, including error handling for missing or inaccessible objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfbe149",
   "metadata": {},
   "source": [
    "### 0. Ensure the AIStore SDK is installed and running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "631c5327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: aistore\n",
      "Version: 1.14.0\n"
     ]
    }
   ],
   "source": [
    "! pip show aistore | grep -E \"Name|Version\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b231487",
   "metadata": {},
   "source": [
    "### 1. Initialize your Client and configure your Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bce86b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from aistore.sdk import Client\n",
    "\n",
    "DEFAULT_ENDPOINT = \"http://localhost:8080\"\n",
    "BCK_NAME = \"get_batch_bck\"\n",
    "\n",
    "# Get endpoint url for AIS cluster\n",
    "ais_url = os.getenv(\"AIS_ENDPOINT\", DEFAULT_ENDPOINT)\n",
    "\n",
    "# Create client and ensure bucket is created\n",
    "# If you get retries, cannot access the cluster\n",
    "client = Client(ais_url)\n",
    "\n",
    "# Clean bucket before creation\n",
    "bucket = client.bucket(BCK_NAME).delete(missing_ok=True)\n",
    "bucket = client.bucket(BCK_NAME).create(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac67b4c",
   "metadata": {},
   "source": [
    "### 2. Populate bucket with a couple basic objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "875958f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-obj-1 37\n",
      "test-obj-2 37\n",
      "test-obj-3 37\n",
      "test-obj-4 37\n",
      "test-obj-5 37\n"
     ]
    }
   ],
   "source": [
    "OBJECT_NAME = \"test-obj\"\n",
    "OBJECT_DATA = b\"This is the data stored in test-obj-\"\n",
    "NUM_OBJECTS = 5\n",
    "\n",
    "objects = []\n",
    "\n",
    "# Create basic test objects\n",
    "for i in range(1, NUM_OBJECTS + 1):\n",
    "    obj = bucket.object(f\"{OBJECT_NAME}-{i}\")\n",
    "    obj.get_writer().put_content(OBJECT_DATA + str(i).encode())\n",
    "\n",
    "    objects.append(obj)\n",
    "\n",
    "# Validate object PUT was successful\n",
    "for entry in bucket.list_all_objects():\n",
    "    print(entry.name, entry.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c10e08",
   "metadata": {},
   "source": [
    "### 3. Initialize batch request spec (`BatchRequest`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d4417ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test-obj-1', 'test-obj-2', 'test-obj-3', 'test-obj-4', 'test-obj-5']\n"
     ]
    }
   ],
   "source": [
    "from aistore.sdk import BatchRequest\n",
    "\n",
    "\"\"\"\n",
    "We're defining the behavior of the batch request:\n",
    "\n",
    "* Output format: `.tar` archive\n",
    "* Continue on errors: skip missing objects instead of failing\n",
    "* Use streaming: return a streamable `.tar` instead of multipart content\n",
    "\n",
    "**Note:** Creating a `BatchRequest` only builds the request spec â€” no data is fetched until you call `get_batch()`.\n",
    "\"\"\"\n",
    "batch_request = BatchRequest(output_format=\".tar\", continue_on_err=True, streaming=True)\n",
    "\n",
    "# Add object to the batch request\n",
    "for obj in objects:\n",
    "    batch_request.add_object_request(obj)\n",
    "\n",
    "\n",
    "# Verify BatchRequest has expected contents\n",
    "print([item[\"objname\"] for item in batch_request.to_dict()[\"in\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04068687",
   "metadata": {},
   "source": [
    "### 4. Use `BatchLoader` to send batch requests to AIStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e0ed543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: test-obj-1, Content: b'This is the data stored in test-obj-1'\n",
      "Name: test-obj-2, Content: b'This is the data stored in test-obj-2'\n",
      "Name: test-obj-3, Content: b'This is the data stored in test-obj-3'\n",
      "Name: test-obj-4, Content: b'This is the data stored in test-obj-4'\n",
      "Name: test-obj-5, Content: b'This is the data stored in test-obj-5'\n"
     ]
    }
   ],
   "source": [
    "# Initialize loader using Client\n",
    "batch_loader = client.batch_loader()\n",
    "\n",
    "# Send the batch request and receive the data\n",
    "batch_iter = batch_loader.get_batch(batch_request)\n",
    "\n",
    "# Validate batch request\n",
    "for resp_item, data in batch_iter:\n",
    "    # The response item will be of type BatchResponseItem\n",
    "    # from aistore.sdk import BatchResponseItem\n",
    "    print(f\"Name: {resp_item.obj_name}, Content: {data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b65cd88",
   "metadata": {},
   "source": [
    "### 5. Uploading data across multiple archive objects in another bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "300b398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BCK_NAME_ARCH = \"get_batch_bck_arch\"\n",
    "\n",
    "# Create second bucket\n",
    "arch_bucket = client.bucket(BCK_NAME_ARCH).delete(missing_ok=True)\n",
    "arch_bucket = client.bucket(BCK_NAME_ARCH).create(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91b5d079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archive-1.tar\n",
      "- archive-1.tar/file_1.txt\n",
      "- archive-1.tar/file_2.txt\n",
      "- archive-1.tar/file_3.txt\n",
      "- archive-1.tar/file_4.txt\n",
      "- archive-1.tar/file_5.txt\n",
      "archive-2.tar\n",
      "- archive-2.tar/file_1.txt\n",
      "- archive-2.tar/file_2.txt\n",
      "- archive-2.tar/file_3.txt\n",
      "- archive-2.tar/file_4.txt\n",
      "- archive-2.tar/file_5.txt\n",
      "archive-3.tar\n",
      "- archive-3.tar/file_1.txt\n",
      "- archive-3.tar/file_2.txt\n",
      "- archive-3.tar/file_3.txt\n",
      "- archive-3.tar/file_4.txt\n",
      "- archive-3.tar/file_5.txt\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "from io import BytesIO\n",
    "\n",
    "# Create tarfile archives\n",
    "NUM_ARCHIVES = 3\n",
    "NUM_FILES = 5\n",
    "\n",
    "ARCH_NAME = \"archive\"\n",
    "FILE_NAME = \"file\"\n",
    "FILE_DATA = b\"This is the data stored in file_\"\n",
    "\n",
    "archive_objs = []\n",
    "\n",
    "for arch_i in range(1, NUM_ARCHIVES + 1):\n",
    "    tar_buffer = BytesIO()\n",
    "\n",
    "    # For each archive, create 5 text files\n",
    "    with tarfile.open(fileobj=tar_buffer, mode=\"w\") as tar:\n",
    "        for file_i in range(1, NUM_FILES + 1):\n",
    "            tarinfo = tarfile.TarInfo(name=f\"{FILE_NAME}_{file_i}.txt\")\n",
    "            tarinfo.size = len(FILE_DATA + str(file_i).encode())\n",
    "            tar.addfile(tarinfo, BytesIO(FILE_DATA + str(file_i).encode()))\n",
    "\n",
    "    # Put archive in AIStore\n",
    "    obj = arch_bucket.object(f\"{ARCH_NAME}-{arch_i}.tar\")\n",
    "    obj.get_writer().put_content(tar_buffer.getvalue())\n",
    "\n",
    "    archive_objs.append(obj)\n",
    "\n",
    "# Validate archives have been PUT\n",
    "for obj in archive_objs:\n",
    "    print(obj.name)\n",
    "    for entry in arch_bucket.list_archive(obj.name):\n",
    "        print(\"-\", entry.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76df2c72",
   "metadata": {},
   "source": [
    "### 6. Update `BatchRequest` with new archive files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90be7c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test-obj-1', 'test-obj-2', 'test-obj-3', 'test-obj-4', 'test-obj-5', 'archive-1.tar', 'archive-2.tar', 'archive-3.tar']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# We can reuse our earlier batch request since they are just specs\n",
    "# Since we want to fetch the other object data too, we're okay to reuse\n",
    "\n",
    "# Add archives to the batch request\n",
    "for obj in archive_objs:\n",
    "    # Get random text file from each archive\n",
    "    random_file_i = random.randint(1, NUM_FILES)\n",
    "    batch_request.add_object_request(obj, archpath=f\"{FILE_NAME}_{random_file_i}.txt\")\n",
    "\n",
    "# Verify BatchRequest has expected contents\n",
    "print([item[\"objname\"] for item in batch_request.to_dict()[\"in\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ec9796",
   "metadata": {},
   "source": [
    "### 7. Fetching data across multiple buckets + types (object AND archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c220366f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: test-obj-1, Archpath: , Content: b'This is the data stored in test-obj-1'\n",
      "Name: test-obj-2, Archpath: , Content: b'This is the data stored in test-obj-2'\n",
      "Name: test-obj-3, Archpath: , Content: b'This is the data stored in test-obj-3'\n",
      "Name: test-obj-4, Archpath: , Content: b'This is the data stored in test-obj-4'\n",
      "Name: test-obj-5, Archpath: , Content: b'This is the data stored in test-obj-5'\n",
      "Name: archive-1.tar, Archpath: file_1.txt, Content: b'This is the data stored in file_1'\n",
      "Name: archive-2.tar, Archpath: file_1.txt, Content: b'This is the data stored in file_1'\n",
      "Name: archive-3.tar, Archpath: file_3.txt, Content: b'This is the data stored in file_3'\n"
     ]
    }
   ],
   "source": [
    "# We can reuse BatchLoader too!\n",
    "batch_iter = batch_loader.get_batch(batch_request)\n",
    "\n",
    "# Validate batch request\n",
    "for resp_item, data in batch_iter:\n",
    "    # The response item will be of type BatchResponseItem\n",
    "    # from aistore.sdk import BatchResponseItem\n",
    "    print(\n",
    "        f\"Name: {resp_item.obj_name}, Archpath: {resp_item.archpath}, Content: {data}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aistore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
